{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Ollama API Inference](#ollama-api)\n",
    "  - [CLI](#cli-inference---ollama-api)\n",
    "  - [Gradio-UI](#gradio-ui---ollama-api)\n",
    "  - [Streamlit-UI](#streamlit-ui---ollama-api)\n",
    "  \n",
    "\n",
    "- [Langchain Inference](#langchain)\n",
    "  - [Initial testing](#initial-testing---langchain)\n",
    "  - [CLI](#cli---langchain)\n",
    "  - [Gradio-UI](#gradio-ui---langchain)\n",
    "  - [Streamlit-UI](#streamlit-ui---langchain)\n",
    "  - [Open-WebUI](#open-webui---langchain)\n",
    "  \n",
    "\n",
    "- [Extras](#extras)\n",
    "  - [HuggingFace API](#huggingface-api)\n",
    "  - [Anthropic API](#anthropic-api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLI Inference - Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I assist you today?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from locallm.response import response_api\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "model=\"llama3.2:1b\"\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "def chat(prompt):\n",
    "    response = response_api(prompt, conversation_history, url=url, model=model, headers=headers)\n",
    "    conversation_history.append(prompt)\n",
    "    conversation_history.append(response)\n",
    "    return response if response else \"Error\"\n",
    "\n",
    "chat(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio UI - Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from locallm.response import response_api\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "model=\"llama3.2:1b\"\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "def chat(prompt):\n",
    "    if prompt.strip().lower() == \"/clear\":\n",
    "        clear_history()\n",
    "    response = response_api(prompt, conversation_history, url=url, model=model, headers=headers)\n",
    "    conversation_history.append(prompt)\n",
    "    conversation_history.append(response)\n",
    "    return response if response else \"Error\"\n",
    "\n",
    "## Gradio UI\n",
    "def clear_history():\n",
    "    conversation_history.clear()  # Reset the list\n",
    "    return \"Chat history cleared.\"\n",
    "\n",
    "# demo = gr.Interface(\n",
    "#     fn=chat,\n",
    "#     inputs=gr.Textbox(lines=2, placeholder=\"Enter your prompt here...\"),\n",
    "#     outputs=\"text\"\n",
    "# )\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    textbox = gr.Textbox(lines=1, placeholder=\"Enter your prompt here...\")\n",
    "    output = gr.Textbox()\n",
    "    submit_btn = gr.Button(\"Send\")\n",
    "    clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "    textbox.submit(chat, inputs=textbox, outputs=output)\n",
    "    submit_btn.click(chat, inputs=textbox, outputs=output)\n",
    "    clear_btn.click(clear_history, inputs=[], outputs=output)\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamlit UI - Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "result = subprocess.run([\"streamlit\", \"run\", \"ui/st_ollama.py\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial testing - Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Canada is Ottawa.\n"
     ]
    }
   ],
   "source": [
    "## Stateless single query\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model=\"llama3.2:1b\"\n",
    "llm = Ollama(model=model) \n",
    "\n",
    "response = llm.invoke(\"What is the capital of Canada?\")\n",
    "# response = llm.invoke(input())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity was not exactly \"discovered.\" Instead, the concept of gravity has been understood and described by various scientists throughout history. Here are some key figures who contributed to our understanding of gravity:\n",
      "\n",
      "1. Sir Isaac Newton (1643-1727): Newton is often credited with developing the law of universal gravitation, which states that every point mass attracts every other point mass by a force acting along the line intersecting both points. He published his groundbreaking work \"Philosophi√¶ Naturalis Principia Mathematica\" in 1687.\n",
      "2. Galileo Galilei (1564-1642): Galileo made significant contributions to our understanding of gravity during his experiments with falling bodies and the behavior of celestial objects. He observed that objects fall at the same rate in all gravitational fields, which challenged Newton's theory of universal gravitation.\n",
      "3. Johannes Kepler (1571-1630): Kepler discovered the law of elliptical orbits, which describes how planets orbit around the sun. This work laid the foundation for our understanding of the gravitational forces that govern planetary motion.\n",
      "4. Albert Einstein (1879-1955): Einstein's theory of general relativity revolutionized our understanding of gravity. He introduced the concept of spacetime and showed that gravity is not a force, but rather the curvature of spacetime caused by massive objects.\n",
      "\n",
      "While these scientists made significant contributions to our understanding of gravity, it's essential to note that the concept of gravity has been described and studied by many other scientists throughout history, including:\n",
      "\n",
      "* Epicurus (341-270 BCE): Proposed the idea that all objects attract each other with an invisible force.\n",
      "* Aristotle (384-322 BCE): Described the attractive force between objects as a result of their motion.\n",
      "* Democritus (460-370 BCE): Proposed the idea that objects move in circular orbits due to the attractive force.\n",
      "\n",
      "The understanding and description of gravity have evolved over time, and scientists continue to refine our understanding of this fundamental force of nature.\n"
     ]
    }
   ],
   "source": [
    "## Multi query\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model=\"llama3.2:1b\"\n",
    "# llm = ChatOllama(\n",
    "    #     model=model,\n",
    "    #     temperature=0.8,\n",
    "    #     num_predict=256\n",
    "    # )\n",
    "llm = ChatOllama(model=model) \n",
    "\n",
    "# content = input()\n",
    "content = \"Who discovered gravity?\"\n",
    "messages = [\n",
    "    HumanMessage(content=content),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The winner of the 2022 FIFA World Cup was Argentina. They defeated France 4-2 in a penalty shootout, after the match ended 3-3 after extra time, to become the first South American team to win the tournament since Brazil in 1970 and 1994.\n"
     ]
    }
   ],
   "source": [
    "## Template \n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"What is the capital of {country}?\"\n",
    "prompt = PromptTemplate(input_variables=[\"country\"], template=prompt_template)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Who won the FIFA World Cup in {year}?\")\n",
    "\n",
    "# Create a RunnableSequence that combines the prompt and LLM\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "# chain = LLMChain(llm=llm, prompt=prompt) ## deprecated\n",
    "\n",
    "# Execute the sequence with input variables\n",
    "response = chain.invoke({\"country\":\"Canada\", \"year\": 2022})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing is a way of processing information that's different from the way computers currently work. Here's a simple explanation:\n",
      "\n",
      "**Classical Computers**\n",
      "\n",
      "Think of a classical computer like a really smart but very slow calculator. It works by using numbers and logic gates to perform calculations. Each gate can either be true (1) or false (0), and it takes time to figure out what the answer is.\n",
      "\n",
      "**Quantum Computers**\n",
      "\n",
      "A quantum computer is similar, but it uses special \"qubits\" (quantum bits). Qubits can exist in multiple states at the same time, like being both 0 and 1 simultaneously. This means a quantum computer can process many calculations at once, much faster than a classical computer.\n",
      "\n",
      "Imagine you have a lot of calculators, each one with its own set of numbers to calculate. A classical computer would need to switch between all those calculators (each one working on the current number) to get the right answer. But a quantum computer can do it all in parallel, like having many calculators that work simultaneously.\n",
      "\n",
      "**Key Features**\n",
      "\n",
      "* **Superposition**: Qubits can exist in multiple states at once.\n",
      "* **Entanglement**: Qubits can be connected in such a way that what happens to one qubit affects the other, even if they're separated by large distances.\n",
      "* **Quantum gates**: Special operations that manipulate qubits.\n",
      "\n",
      "**Benefits**\n",
      "\n",
      "* **Speedup**: Quantum computers can solve certain problems much faster than classical computers.\n",
      "* **Improved accuracy**: Quantum computers can be used for tasks like simulating molecules and optimizing complex systems.\n",
      "* **New possibilities**: Quantum computing has the potential to revolutionize fields like cryptography, materials science, and optimization.\n",
      "\n",
      "While quantum computing is still in its early stages, researchers are making rapid progress in understanding how it works. It's an exciting area of research with many potential applications!"
     ]
    }
   ],
   "source": [
    "## Streaming ##\n",
    "for chunk in llm.stream(\"Explain quantum computing in simple terms.\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "# Or use chain.stream with a template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ChatPromptTemplate\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful and friendly AI assistant. Respond conversationally and helpfully. Here is the conversation history: {chat_history}\"),\n",
    "    (\"user\", \"{user_input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='How does machine learning work?')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[1]\n",
    "# HumanMessage(content=user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is such a broad and fascinating field, but I'll try to break it down in simple terms.\n",
      "\n",
      "**What is Machine Learning?**\n",
      "\n",
      "Machine learning (ML) is a type of artificial intelligence (AI) that enables computers to learn from data without being explicitly programmed. It's all about training algorithms to make predictions or decisions based on patterns in data.\n",
      "\n",
      "**How Does Machine Learning Work?**\n",
      "\n",
      "Here's a simplified overview:\n",
      "\n",
      "1. **Data Collection**: Gathering large amounts of data, which can come in various forms such as images, text, audio, or sensor readings.\n",
      "2. **Data Preprocessing**: Cleaning, transforming, and preparing the data for analysis.\n",
      "3. **Model Selection**: Choosing an algorithm (a set of instructions) to use for learning from the data.\n",
      "4. **Training**: The algorithm is fed the training data, and it starts to learn patterns and relationships between variables.\n",
      "5. **Validation**: Testing the model on a separate dataset to evaluate its performance and accuracy.\n",
      "6. **Deployment**: Using the trained model in real-world applications, such as making predictions or decisions.\n",
      "\n",
      "**Types of Machine Learning**\n",
      "\n",
      "There are several types of machine learning, including:\n",
      "\n",
      "1. **Supervised Learning**: The algorithm learns from labeled data (e.g., images with correct labels).\n",
      "2. **Unsupervised Learning**: The algorithm discovers patterns and relationships in unlabeled data.\n",
      "3. **Reinforcement Learning**: The algorithm learns through trial and error, interacting with an environment to maximize rewards.\n",
      "\n",
      "**How Do Algorithms Learn?**\n",
      "\n",
      "Algorithms learn from data by adjusting their parameters or weights. This process is called optimization. As the algorithm processes more data, it refines its understanding of patterns and relationships, leading to improved performance.\n",
      "\n",
      "**What's the Goal of Machine Learning?**\n",
      "\n",
      "The ultimate goal of machine learning is to enable computers to make accurate predictions, decisions, or recommendations based on complex data patterns.\n",
      "\n",
      "I hope this helps you understand the basics of machine learning! Do you have any specific questions or areas you'd like me to expand upon?"
     ]
    }
   ],
   "source": [
    "user_input = \"How does machine learning work?\"\n",
    "chat_history = []\n",
    "messages = chat_prompt.format_messages(user_input=user_input, chat_history=chat_history)\n",
    "\n",
    "# response = chat_model.invoke(messages)\n",
    "# print(response.content)\n",
    "\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Integrating chat history\n",
    "\n",
    "def chat(user_input, llm=llm, chat_history=[], out=True):\n",
    "    messages = chat_prompt.format_messages(user_input=user_input, chat_history=chat_history)\n",
    "    chat_history.append(HumanMessage(content=user_input))  \n",
    "\n",
    "    response = llm.invoke(messages)  \n",
    "    if out:\n",
    "        response = []\n",
    "        for chunk in llm.stream(messages):\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            response.append(chunk.content)\n",
    "        print()\n",
    "        response = \" \".join(response)\n",
    "    else:\n",
    "        response = llm.invoke(messages).content\n",
    "    \n",
    "    chat_history.append(AIMessage(response))  \n",
    "    return response, chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Sam! It's nice to meet you. Is there something I can help you with, or would you like to chat for a bit?\n",
      "###\n",
      "Here's one: A man walked into a library and asked the librarian, \"Do you have any books on Pavlov's dogs and Schr√∂dinger's cat?\" The librarian replied, \"It rings a bell, but I'm not sure if it's here or not.\"\n",
      "###\n",
      "The joke is a play on words. Pavlov's dogs refers to Ivan Pavlov, a Russian scientist who conditioned animals to associate the sound of a bell with food. The idea is that if Pavlov's dogs were in a library, they might be looking for books about dog behavior.\n",
      "\n",
      "Schr√∂dinger's cat, on the other hand, is a thought experiment by Austrian physicist Erwin Schr√∂dinger. It's a famous example of quantum superposition, where a cat can exist in multiple states (alive and dead) until its state is observed.\n",
      "\n",
      "The punchline \"it rings a bell\" is a clever connection between Pavlov's dogs and the idea that Schr√∂dinger's cat might be in the library book you're looking for. The librarian is saying that the title sounds familiar, but they're not sure if it's actually in the library or just ringing a bell (like Pavlov's dogs) because of the similar name.\n",
      "\n",
      "So, the joke relies on wordplay and clever connections between two famous scientific concepts to create a humorous effect!\n",
      "###\n",
      "Your name is Sam! I remember our conversation earlier when we started chatting. How are you doing today, Sam? Is there anything else I can help you with or would you like to keep chatting?\n"
     ]
    }
   ],
   "source": [
    "response, chat_history = chat(\"Hello! This is Sam\")\n",
    "print(\"###\")\n",
    "response, chat_history = chat(\"Tell me a joke.\")\n",
    "print(\"###\")\n",
    "response, chat_history = chat(\"Can you explain it?\")\n",
    "print(\"###\")\n",
    "response, chat_history = chat(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello! This is Sam'),\n",
       " AIMessage(content=\"Hi  Sam !  It 's  nice  to  meet  you .  Is  there  something  I  can  help  you  with ,  or  would  you  like  to  chat  for  a  bit ? \"),\n",
       " HumanMessage(content='Tell me a joke.'),\n",
       " AIMessage(content='Here \\'s  one :  A  man  walked  into  a  library  and  asked  the  librarian ,  \" Do  you  have  any  books  on  Pav lov \\'s  dogs  and  Sch r √∂ d inger \\'s  cat ?\"  The  librarian  replied ,  \" It  rings  a  bell ,  but  I \\'m  not  sure  if  it \\'s  here  or  not .\" '),\n",
       " HumanMessage(content='Can you explain it?'),\n",
       " AIMessage(content='The  joke  is  a  play  on  words .  Pav lov \\'s  dogs  refers  to  Ivan  Pav lov ,  a  Russian  scientist  who  conditioned  animals  to  associate  the  sound  of  a  bell  with  food .  The  idea  is  that  if  Pav lov \\'s  dogs  were  in  a  library ,  they  might  be  looking  for  books  about  dog  behavior .\\n\\n Sch r √∂ d inger \\'s  cat ,  on  the  other  hand ,  is  a  thought  experiment  by  Austrian  physicist  Er win  Sch r √∂ d inger .  It \\'s  a  famous  example  of  quantum  super position ,  where  a  cat  can  exist  in  multiple  states  ( alive  and  dead )  until  its  state  is  observed .\\n\\n The  punch line  \" it  rings  a  bell \"  is  a  clever  connection  between  Pav lov \\'s  dogs  and  the  idea  that  Sch r √∂ d inger \\'s  cat  might  be  in  the  library  book  you \\'re  looking  for .  The  librarian  is  saying  that  the  title  sounds  familiar ,  but  they \\'re  not  sure  if  it \\'s  actually  in  the  library  or  just  ringing  a  bell  ( like  Pav lov \\'s  dogs )  because  of  the  similar  name .\\n\\n So ,  the  joke  relies  on  word play  and  clever  connections  between  two  famous  scientific  concepts  to  create  a  humorous  effect ! '),\n",
       " HumanMessage(content='What is my name?'),\n",
       " AIMessage(content='Your  name  is  Sam !  I  remember  our  conversation  earlier  when  we  started  chatting .  How  are  you  doing  today ,  Sam ?  Is  there  anything  else  I  can  help  you  with  or  would  you  like  to  keep  chatting ? ')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLI - Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Sam! Nice to meet you. How's your day going so far?\n",
      "You said your name in the initial message. Your name is Sam. How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "from locallm.response import response_lc\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "model=\"llama3.2:1b\"\n",
    "llm = ChatOllama(model=model)\n",
    "chat_history=[]\n",
    "def chat(user_input):\n",
    "    global chat_history\n",
    "    response, chat_history = response_lc(user_input, chat_history=chat_history, llm=llm, out=True)\n",
    "    return response\n",
    "\n",
    "_ = chat(\"Hello, I'm Sam\")\n",
    "_ = chat(\"What's my name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio UI - Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from locallm.response import response_lc\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "model=\"llama3.2:1b\"\n",
    "llm = ChatOllama(model=model)\n",
    "chat_history=[]\n",
    "\n",
    "def chat(user_input):\n",
    "    if user_input.strip().lower() == \"/clear\":\n",
    "        return clear_history()\n",
    "    global chat_history\n",
    "    response, chat_history = response_lc(user_input, chat_history=chat_history, llm=llm, out=False)\n",
    "    return response\n",
    "\n",
    "## Gradio UI\n",
    "def clear_history():\n",
    "    global chat_history\n",
    "    chat_history.clear()  # Reset the list\n",
    "    return \"Chat history cleared.\"\n",
    "\n",
    "# demo = gr.Interface(\n",
    "#     fn=chat,\n",
    "#     inputs=gr.Textbox(lines=1, placeholder=\"Enter your prompt here...\"),\n",
    "#     outputs=\"text\"\n",
    "# )\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    textbox = gr.Textbox(lines=1, placeholder=\"Enter your prompt here...\")\n",
    "    output = gr.Textbox()\n",
    "    submit_btn = gr.Button(\"Send\")\n",
    "    clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "    textbox.submit(chat, inputs=textbox, outputs=output)\n",
    "    submit_btn.click(chat, inputs=textbox, outputs=output)\n",
    "    clear_btn.click(clear_history, inputs=[], outputs=output)\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamlit UI - Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "result = subprocess.run([\"streamlit\", \"run\", \"ui/st_langchain.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open WebUI - Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jello molds with fruit and nuts are a classic summer dessert. These easy-to-make desserts are perfect for potlucks, barbecues, and outdoor gatherings. Here are some creative ideas to take your Jello molds to the next level:\\n\\n1.  Tropical fruit salad: Mix together your favorite tropical fruits like pineapple, mango, and kiwi, and add some toasted coconut flakes for extra texture and flavor.\\n2.  Edible flowers: Add some edible flowers like violas, pansies, or nasturtiums to give your Jello molds a colorful and whimsical touch.\\n3.  Fresh herbs: Incorporate fresh herbs like mint, basil, or lemongrass into your Jello molds for a refreshing and fragrant twist.\\n4.  Citrus zest: Add some grated citrus zest like orange, lemon, or lime to the Jello mixture for a burst of citrus flavor.\\n5.  Chopped nuts: Mix in some chopped nuts like almonds, walnuts, or pecans for added crunch and texture.\\n6.  Candy pieces: Add some candy pieces like M&M's, chopped peanut butter cups, or chopped candy canes for a fun and playful touch.\\n7.  Fruit preserves: Mix in some fruit preserves like raspberry or strawberry jam for a sweet and tangy flavor.\\n8.  Whipped cream: Top your Jello molds with whipped cream and a sprinkle of sugar for a light and airy finish.\\n\\nHere's a basic Jello mold recipe to get you started:\\n\\nIngredients:\\n\\n*   1 cup boiling water\\n*   1 envelope (0.25 oz) unsweetened gelatin\\n*   1 cup granulated sugar\\n*   1 cup cold water\\n*   1 cup fruit (such as strawberries, blueberries, or raspberries)\\n*   1/4 cup chopped nuts (optional)\\n*   1/4 cup whipped cream (optional)\\n\\nInstructions:\\n\\n1.  In a small bowl, sprinkle the gelatin over the cold water and let it sit for 2-3 minutes to soften.\\n2.  In a large bowl, combine the sugar, boiling water, and softened gelatin. Stir until the sugar has dissolved.\\n3.  Add the fruit and chopped nuts (if using) to the bowl and stir until well combined.\\n4.  Pour the mixture into a 9x13 inch pan or a mold of your choice.\\n5.  Refrigerate the mixture for at least 3 hours or until set.\\n6.  Once set, remove the Jello mold from the refrigerator and top with whipped cream (if using).\\n7.  Serve chilled and enjoy!\\n\\nThese creative Jello mold ideas will add a fun and unique twist to your summer desserts. Experiment with different ingredients and flavors to create your own signature Jello molds. Happy baking!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Setup: Load HUGGINGFACEHUB_API_TOKEN saved in .env \n",
    "\n",
    "## Replace following:\n",
    "# model=\"llama3.2:1b\"\n",
    "# llm = ChatOllama(model=model)\n",
    "\n",
    "## With:\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",  # Replace with your desired model\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 100},\n",
    ")\n",
    "\n",
    "## Now this can be used with any UI \n",
    "\n",
    "llm.invoke(\"Jello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can use Anthropic API\n",
    "\n",
    "Setup:\n",
    "Install langchain-anthropic and set environment variable ANTHROPIC_API_KEY.\n",
    "\n",
    "pip install -U langchain-anthropic\n",
    "export ANTHROPIC_API_KEY=\"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "llm = ChatAnthropic(model_name=\"claude-3-haiku-20240307\")\n",
    "\n",
    "# Can use this llm with our methods and UI above\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio UI - Langchain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
